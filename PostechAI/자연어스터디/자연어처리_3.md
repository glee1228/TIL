## 두번째 수업

: 지난 첫번째 수업 했던 내용을 복습한다.



## 문자열 처리

- 위에서 간략하게 살펴 본 내용을 바탕으로 문자열 처리해본다.

```python
def review_to_words(raw_review):
    #1. HTML 제거
    review_text = BeautifulSoup(raw_review,'html.parser').get_text()
    #2. 영문자가 아닌 문자는 공백으로 변환
    letters_only = re.sub('[^a-zA-Z]',' ',review_text)
    #3. 소문자 변환
    words = letters_only.lower().split()
    #4. 파이썬에서는 리스트보다 세트로 찾는게 훨씬 빠르다.
    # stopwords 를 세트로 변환한다.
    stops = set(stopwords.words('english'))
    # 5. Stopwords 불용어 제거
    meaningful_words = [w for w in words if not w in stops]
    # 6. 어간 추출
    stemming_words = [stemmer.stem(w) for w in meaningful_words]
    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환
    return( ' ',join(stemming_words))
```

```python
clean_review = review_to_words(train['review'][0])
```



## 전체 텍스트 데이터 처리

: 첫 번째 리뷰를 대상으로 전처리 해줬던 내용을 전체 텍스트 데이터를 대상으로 처리한다.

#### 전체 리뷰 데이터 수 가져오기

```python
num_reviews = train['review'].size
print(num_reviews)
```



##### Clean_train_reviews 리스트에 전처리하는 함수의 return 문자열을 삽입

```python
for i in range(0, num_reviews):
    clean_train_reviews.append(review_to_words(train['review'][i]))
```



##### 어느정도 실행되고 있는지 확인하기 위해 5000개 단위로 상태를 찍도록 개선

```python
clean_train_reviews=[]
for i in range(0,num_reviews):
    if(i+1)%5000 ==0:
        print('Review {} of {}'.format(i+1,num_reviews))
    clean_train_reviews.append(review_to_words(train['review'][i]))
```



#### 코드를 간결하게 하기 위해서 apply 함수를 사용

```python
train['review_clean'] = train['review'].apply(review_to_words)
```

:이렇게 하더라도 여전히 오래 걸린다.

CPU Times : user 1min 15s, sys: 2.3 s , total: 1min 18s

Wall time : 1min 20s



## 멀티프로세싱 

```python
# 참고 : https://gist.github.com/yong27/7869662
# https://www.racketracer.com/2016/07/06/pandas-in-parallel/
from multiprocessing import Pool
import numpy as np

def _apply_df(args):
    df, func, kwargs = args
    return df.apply(func, **kwargs)

def apply_by_multiprocessing(df, func, **kwargs):
    #키워드 항목 중 workers 파라미터를 꺼냄
    workers = kwargs.pop('workers')
    # 위에서 가져온 workers 수로 프로세스 풀을 정의
    pool = Pool(processes=workers)
    # 실행할 함수와 데이터프레임을 워커의 수 만큼 나눠 작업
    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])
    pool.close()
    return pd.concat(list(result))
if __name__ == '__main__':
    %time clean_train_reviews = apply_by_multiprocessing(train['review'], review_to_words,workers=4)
```

```python
%time clean_train_reviews = apply_by_multiprocessing(train['review'], review_to_words,workers=4)
```

: Workers : 4 일 경우, 60% 정도 향상된 성능을 볼 수 있다.

```python
%time clean_test_reviews = apply_by_multiprocessing(test['review'], review_to_words,workers=4)
```





## 워드클라우드

* 단어의 빈도 수 데이터를 가지고 있을 때 이용할 수 있는 시각화 방법
* 단순히 빈도 수를 표현하기 보다는 상관관계나 유사도 등으로 배치하는 게 더 의미있기 때문에 큰 정보를 얻기는 어렵다.

```python
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
%matplotlib inline

def displayWordCloud(data = None, backgroundcolor = 'white' width=800,height=800):
    wordcloud = WordCloud(stopwords = STOPWORDS,background_color = backgroundcolor,width=width,height=height).generate(data)
    plt.figure(figsize = (15,10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()
```

* 트레인 데이터에 대한 워드클라우드 그리기

```python
%time displayWordCloud(' '.join(clean_train_reviews))
```



* 테스트 데이터에 대한 워드클라우드 그리기

```python
%time displayWordCloud(' '.join(clean_test_reviews))
```



## 중복 단어 제거

* 단어 수

```python
train['num_words'] = clean_train_reviews.apply(lambda x : len(str(x).split()))
```

* 중복을 제거한 단어 수

```python
train['num_words'] = clean_train_reviews.apply(lambda x : len(set(str(x).split())))
```



* 첫 번째 리뷰에 

```python
x = clean_train_reviews[0]
x = str(x).split()
print(len(x))
print(x[:10])
#219
```



* 리뷰별 단어 평균 개수, 리뷰별 고유 단어 평균 개수

```python
import seaborn as sns
#Seaborn : Matplotlib 기반으로 색상 테마와 통계용 차트 등의 기능을 추가한 패키지

fig,axes = plt.subplot(ncols=2)
fig.set_size_inches(18,6)
print('리뷰별 단어 평균 값 :',train['num_words'].mean())
print('리뷰별 단어 중간 값 :',train['num_words'].median())
sns.distplot(train['num_words'],bins=100,ax=axes[0])
axes[0].axvline(train['num_words'].median(),linestyle='dashed')
axes[0].set_title('리뷰별 고유 단어 수 분포')

print('리뷰별 고유 단어 평균 값 :',train['num_uniq_words'].mean())
print('리뷰별 고유 단어 중간 값 :',train['num_uniq_words'].median())
sns.distplot(train['num_uniq_words'],bins=100,color='g',ax=axes[1])
axes[1].axvline(train['num_uniq_words'].median(), linestyle='dashed')
axes[1].set_title('리뷰별 고유한 단어 수 분포')


```

전처리된 데이터를 바탕으로 워드클라우드로 시각화 하는 작업을 했습니다.



## 텍스트데이터 벡터화

다음의 두 문장이 있다고 하자,

```
1) John likes to watch movies. Mary movies too.
2) John also likes to watch football games.
```

```
#위 문장을 토큰화 하여 가방에 담아주면 다음과 같다.
[
    "John",
    "likes",
    "to",
    "watch",
    "movies",
    "Mary",
    "too",
    "also",
    "football",
    "games"
]
```

그리고 배열의 순서대로 가방에서 각 토큰이 몇 번 등장하는지를 횟수를 세어준다.

```
(1)[1,2,1,1,2,1,1,0,0,0]
(2)[1,1,1,1,0,0,0,1,1,1]
```

=> 머신러닝 알고리즘이 이해할 수 있는 형태로 바꿔주는 작업이다.

단어 가방을 n-gram을 사용해 bigram으로 담아주면 다음과 같다.

```
[
    "John likes",
    "likes to",
    "to watch",
    "watch movies",
    "Mary likes",
    "likes movies",
    "movies too"
]
```

=> 여기에서는 CountVectorizer를 통해 위 작업을 한다.



## 사이킷런의 CountVectorizer를 통해 피처 생성

* 정규표현식을 사용해 토큰을 추출한다.
* 모두 소문자로 변환시키기 때문에 good, Good, gOod이 모두 같은 특성이 된다.(앞에서 함)
* 의미없는 특성을 많이 생성하기 때문에 적어도 두 개의 문서에 나타난 토큰만을 사용한다.
* min_df로 토큰이 나타날 최소 문서 개수를 지정할 수 있다.



```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipline import Pipeline

# 튜토리얼과 다르게 파라미터 값을 수정
# 파라미터 값만 수정해도 캐글 스코어 차이가 많이 남
vectorizer = CountVectorizer(analyzer='word',
                            tokenizer=None,
                            preprocessor=None,
                            stop_words=None,
                            min_df=2, #토큰이 나타날 최소 문서 개수
                            ngram_range=(1,3),
                            max_feature=20000
                            )
print(vectorizer)
```



```python
# 속도 개선을 위해 파이프라인을 사용하도록 개선
# 참고 : https://stackoverflow.com/questions/28160335/plot-a-document-tfidf-2d-graph
pipline = Pipeline([
    ('vect',vectorizer),
])
%time train_data_features =pipeline.fit_transform(clean_train_reviews)
print(train_data_features)
print(train_data_features.shape)

```

: 25000개의 행수와 20000개의 칼럼데이터로 생성

```python
vocab = vectorizer.get_feature_names()
print(len(vocab))
print(vocab[:10])
```



## 벡터화된 피쳐를 확인

:각 단어들이 문장에서 몇번 등장했는지 확인

```python
import numpy as np
dist = np.sum(train_data_features,axis=0)

for tag,count in zip(vocab,dist):
    print(count,tag)
featured_train_vector=pd.DataFrame(dist,columns=vocab)
print(featured_train_vector)
```

```python
print(pd.DataFrame(train_data_features[:10].toarray(),columns=vocab).head())
```



## 랜덤포레스트

: test데이터에 있는 데이터가 긍정인지 부정인지 추측해본다.

* 분류(Classification), 회귀 분석(Regression) 등 지도학습에 사용
* 배깅, 임의 노드 최적화, 앙상블 모델을 사용해 모델을 구성함.
* 일반적으로 Decision Tree를 이용한 방법의 경우, 그 결과 또는 성능의 변동 폭이 크다는 결점을 갖고 있음. 특히 결정 트리는 계층적 접근 방식이기 때문에 중간에 에러가 발생하면 다음단계로 에러가 계속 전파되는 특성을 가진다. 
* 배깅(Bagging) 또는 임의 노드 최적화(Randomized node optimization)와 같은 임의화 기술이 결정트리가 가진 단점을 극복하고 좋은 일반화 성능을 갖게 함.

* 랜덤포레스트의 가장 핵심적인 특징은 임의성(randomness)에 의해서 서로 조금씩 다른 특성을 갖는 트리들로 구성된다는 점이다.
* 이 특징은 각 트리들의 예측(prediction)들이 비상관화(deccorrelation)되게 하며, 결과적으로 일반화(generalization) 성능을 향상시킨다. 또한 임의화(randomization)는 포레스트가 노이즈가 포함된 데이터에 대해서도 강인하게 만들어준다.



```python
from sklearn.ensemble import RandomForestClassifier

#랜덤포레스트 분류기를 사용
forest = RandomForestClassifier(n_estimators = 100,n_jobs = -1,random_state=2018)
print(forest)

```



![scikit learn algorithm cheat sheetì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](http://scikit-learn.org/stable/_static/ml_map.png)



## Supervised Machine Learning

https://github.com/amueller/odscon-2015

![supervised Machine learningì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](https://www.mathworks.com/content/mathworks/www/en/discovery/machine-learning/jcr:content/mainParsys3/discoverysubsection_1965078453/mainParsys3/image_2128876021_cop.adapt.full.high.svg/1531721829647.svg)





: 지도학습과 비지도학습의 가장 큰 차이점은 라벨링이 되어있는지 아닌지이다.

트레이닝 데이터는 벡터화된 데이터에 해당되고 센티멘트 데이터는 라벨에 넣어서 모델에 둘다 넣어 학습시킨다.

테스트 데이터는 트레인과 똑같이 벡터화 해서 예측을 하고 예측한 데이터를 평가를 한다.

평가할 때는 ROC커브를 사용해서 평가한다.

지도학습의 큰 맥락은 트레인과 트레인 라벨을 학습하고

테스트의 데이터를 넣어 예측하고 평가하는 과정이 큰 맥락이다.

```
clf = RandomForestClassifier()
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
clf.score(X_test,y_test)
```

: 랜덤포레스트를 사용

랜덤포레스트를 학습

X트레인은 행렬, Y트레인은 벡터데이터가 들어감

count_vectorizer로 벡터화한 데이터를 x_train에 넣고, sentiment를 y_train에 넣는다.



## Decision Trees

![ê´ë ¨ ì´ë¯¸ì§](https://cdn-images-1.medium.com/max/1200/1*XMId5sJqPtm8-RIwVVz2tg.png)

* Decision Tree의 장점을 모아 만든것이 랜덤포레스트
* 여러개의 Decision Tree를 만들어서 평균을 내거나, 투표를 해서 결정을 하는것이 랜덤포레스트 알고리즘이다.
* Depth를 어떻게 지정하냐에 따라서 underfitting이 되기도 하고 overfitting이 되기도 한다.



## Overfitting and Underfitting

![overfitting underfitting graphì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](https://i.ytimg.com/vi/dBLZg-RqoLg/maxresdefault.jpg)

* 과소적합 : 너무 간단한 모델이 선택되어 데이터의 특징과 다양성을 잡아내지 못함
* 과대적합 : 너무 복잡한 모델을 만들어 일반화 하기 어려움



```python
from sklearn.ensemble import RandomForestClassifier

#랜덤포레스트 분류기를 사용
#n_estimators : 숫자를 크게 지정할 수록 좀더 좋은 성능을 냄
#n_jobs : 가지고 있는 장비의 cpu코어를 모두 사용 -1:모든 코어 사용 2,3: 두개나 세개
#random_state : 파라미터를 튜닝했을 때 같은 스코어가 어떻게 달라진건지 명확히 알수있기 때문에 지정함.
forest = RandomForestClassifier(n_estimators = 100,n_jobs = -1,random_state=2018)
print(forest)
```

```python
%time forest = forest.fit(train_data_features,train['sentiment'])
```

: 행렬 데이터와 벡터데이터를 삽입

```python
from sklearn.model_selection import cross_val_score
%time score = np.mean(cross_val_score(forest,train_data_features,train['sentiment'],cv=10,scoring='roc_auc'))
```



```python
test_data_features = pipline.transform(clean_test_reviews)
test_data_features = test_data_features.toarray()
print(test_data_features)
```

### 벡터화된 단어로 숫자가 문서에서 등장하는 횟수 보기

```python
print(test_data_features[5][:100])
```

: 사전에서 ''이 단어가 한번등장한다 두번등장한다''를 해당 인덱스로 보는 것이 가능하다.

```python
#벡터화 하며 만든 사전에서 해당 단어가 무엇인지 찾아볼 수 있다.
# vocab = vectorizer.get_feature_names()
print(vocab[8],vocab[2558],vocab[2559],vocab[2560])
```



```python
#테스트 데이터를 넣고 예측한다.
result = forest.predict(test_data_features)
print(result[:10])
```



```python
#예측 결과를 저장하기 위해 데이터프레임에 담아준다.
output = pd.DataFrame(data={'id':test['id'],'sentiment':result})
output.head()
```



```python
output.to_csv('data/tutorial_1_BOW_{0:.5f}.csv'.format(score),index=False,quoting=3)
```



## 결과 확인

```python
output_sentiment = output['sentiment'].value_counts()
print(output_sentiment[0]-output_sentiment[1])
print(ouput_sentiment)
```

* 총 12500개 중 108개 의 데이터가 차이가 있다.

```python
fig,axes = plt.subplots(ncols=2)
sns.countplot(train['sentiment'],ax=axes[0])
sns.countplot(test['sentiment'],ax=axes[1])
```

```python
#파라미터를 조정해가며 점수를 조금씩 올려본다.

#uni-gram 사용 시, 캐글 점수 0.84476
print(476/578)
#tri-gram 사용 시, 캐글 점수 0.84608
print(388/578)
#어간 추출 후 캐글 점수 0.84870
print(339.578)
#랜덤포레스트의 max_depth =5 로 지정하고 
#CountVectorizer의 tokenizer=nltk.word_tokenize 를 지정후 캐글 점수 0.8146
print(546/578)
#랜덤포레스트의 max_depth=5는 다시 None으로 변경
#CountVectorizer max_features = 10000개로 변경 후 캐글 점수 0.85272
print(321/578)
#CountVectorizer의 tokenizer=nltk.word_tokenize 를 지정 후 캐글 점수 0.8504
print(326/578)
#CountVectorizer max_features = 10000개로 변경 후 캐글 점수 0.85612
print(305/578)
#현재 상태
print(310/578)
```

