## 자연어 처리 -위키 백과

* 자연어 처리**(自然語處理) 또는 **자연 언어 처리**(自然言語處理)는 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 모사 할수 있도록 연구하고 이를 구현하는 [인공지능](https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5)의 주요 분야 중 하나다. 

* 자연 언어 처리는 연구 대상이 언어 이기 때문에 당연하게도 언어 자체를 연구하는 언어학과 언어 현상의 내적 기재를 탐구하는 언어 인지 과학과 연관이 깊다. 구현을 위해 수학적 통계적 도구를 많이 활용하며 특히 기계학습 도구를 많이 사용하는 대표적인 분야이다. 
* 정보검색, QA 시스템, 문서 자동 분류, 신문기사 클러스터링, 대화형 Agent 등 다양한 응용이 이루어 지고 있다.



### 자연어처리(NLP)와 관련된 캐글 경진대회

[Sentiment Analysis on Movie Reviews | Kaggle](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)

<a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Toxic Comment Classification Challenge | Kaggle</a>

<a href="https://www.kaggle.com/c/spooky-author-identification">Spooky Author Identification | Kaggle</a>



### 튜토리얼 개요

#### 파트1 

* 초보자를 대상으로 기본 자연어 처리를 다룬다.

#### 파트 2,3

* Word2Vec을 사용하여 모델을 학습시키는 방법과 감정분석에 단어 벡터를 사용하는 방법을 본다
* 파트 3 은 레시피를 제공하지 않고 Word2Vec을 사용하는 몇가지 방법을 실험해본다.
* 파트3에서는 K-means 알고리즘을 사용해 군집화를 해본다
* 긍정과 부정 리뷰가 섞여있는 100,000개의 IMDB감정분석 데이터 셋을 통해 목표를 달성해본다.



### 평가 - ROC 커브(Receiver-Operating Characteristic curve)

* TPR(True Positive Rate)과 FPR(False Positive Rate)을 각각 x,y 축으로 놓은 그래프
* 민감도 TPR
  * 1인 케이스에 대해 1로 예측한 비율
  * 암환자를 진찰해서 암이라고 진단함
* 특이도 FPR
  * 0인 케이스에 대해 1로 잘못 예측한 비율
  * 암환자가 아닌데 암이라고 진단함
* X,Y가 둘다 [0,1] 범위이고, (0,0)에서 (1,1)을 잇는 곡선이다.
* ROC 커브의 및 면적이 1에 가까울 수록 (왼쪽 꼭지점에 다가갈수록 ) 좋은 성능
* 참고 :
  * ![ROC ì"¤ë¸ì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](http://www.cbgstat.com/method_ROC_curve/images/ROC_curve_Snap8.gif)



### Google's Word2Vec for movie reviews

* 자연어 텍스트를 분석해서 특정 단어를 얼마나 사용했는지, 얼마나 자주 사용했는지, 어떤 종류의 텍스트인지 분류하거나 긍정인지 부정인지에 대한 감정분석, 그리고 어떤 내용인지 요약하는 정보를 얻을 수 있다.
* 감정 분석은 머신러닝에서 어려운 주제로 풍자, 애매모호한 말 ,반어법 , 언어 유희로 표현을 하는데, 이는 사람과 컴퓨터에게 모두 오해의 소지가 있다. 여기에서는 Word2Vec을 통한 감정분석을 해보는 튜토리얼을 해본다.
* Google의 Word2Vec은 단어의 의미와 관계를 이해하는데 도움
* 상당수의 NLP 기능은 nltk모듈에 구현되어있는데, 이 모듈은 코퍼스 , 함수와 알고리즘으로 구성되어 있다.
* 단어 임베딩 모형 테스트 : Korean Word2Vec  
* http://w.elnn.kr/search/



### BOW(bag of words)

https://en.wikipedia.org/wiki/Bag-of-words_model

* 가장 간단하지만 효과적이라 널리쓰이는 방법
* 장, 문단, 문장, 서식과 같은 입력 텍스트의 구조를 제외하고 각 단어가 이 말뭉치에 얼마나 많이 나타나는지만 헤아린다.
* 구조와 상관없이 단어의 출현 횟수만 세기 때문에 텍스트를 담는 가방(Bag)으로 생각할 수 있다.
* BOW는 단어의 순서가 완전히 무시 된다는 단점이 있다. 예를 들어, 의미가 완전히 반대인 두 문장이 있다고 하자
  * ```it's bad, not good at all. ```
  * ```ìt's good, not bad at all.```
* 위 두 문장은 의미가 전혀 반대지만 완전히 동일하게 반환된다.
* 이를 보완하기 위해 n-gram을 사용하는 데 BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용할 수 있도록 한다.



