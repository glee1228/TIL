import pandas as pd
from bs4 import BeautifulSoup
import re
import nltk

from nltk.corpus import stopwords
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
stemmer = nltk.stem.PorterStemmer()

def review_to_words(raw_review):
    #1. HTML 제거
    review_text = BeautifulSoup(raw_review,'html.parser').get_text()
    #2. 영문자가 아닌 문자는 공백으로 변환
    letters_only = re.sub('[^a-zA-Z]',' ',review_text)
    #3. 소문자 변환
    words = letters_only.lower().split()
    #4. 파이썬에서는 리스트보다 세트로 찾는게 훨씬 빠르다.
    # stopwords 를 세트로 변환한다.
    stops = set(stopwords.words('english'))
    # 5. Stopwords 불용어 제거
    meaningful_words = [w for w in words if not w in stops]
    # 6. 어간 추출
    stemming_words = [stemmer.stem(w) for w in meaningful_words]
    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환
    return( ' '.join(stemming_words))
"""
header =0 은 파일의 첫 번째 줄에 열 이름이 있음을 나타내며
delimiter = \t 는 필드가 탭으로 구분되는 것을 의미한다.
quoting =3 은 쌍따옴표를 무시하도록 한다.
"""
# QUOTE_MINIMAL (0), QUOTE_ALL (1),
# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).

# 레이블인 sentiment 가 있는 학습 데이터
train = pd.read_csv('../data/labeledTrainData.tsv',header=0,delimiter='\t',quoting=3)

# 레이블이 없는 테스트 데이터
test = pd.read_csv('../data/testData.tsv',header=0,delimiter='\t',quoting=3)

num_reviews = train['review'].size
print(num_reviews)
clean_review = review_to_words(train['review'][0])
#clean_train_reviews = []

#for i in range(0, num_reviews):
#    clean_train_reviews.append(review_to_words(train['review'][i]))