## 시작하기 전에

강의자료 : 

# [[NLP\] IMDB 영화리뷰 감정 분석을 통한 파이썬 자연어 처리](https://www.inflearn.com/course/nlp-imdb-%ed%8c%8c%ec%9d%b4%ec%8d%ac-%ec%9e%90%ec%97%b0%ec%96%b4-%ec%b2%98%eb%a6%ac/)



pandas 패키지가 다운로드 안되어있다면,

```
$ pip3 install pandas
```

로 pandas 패키지를 다운로드

이외에

```
$ pip3 install numpy
$ pip3 install scikit-learn
$ pip3 install NLTK
$ pip3 install html5lib
```





![스크린샷 2018-09-13 오후 8.48.24](/Users/donghoon/Desktop/스크린샷 2018-09-13 오후 8.48.24.png)

```python
import pandas as pd

"""
header =0 은 파일의 첫 번째 줄에 열 이름이 있음을 나타내며
delimiter = \t 는 필드가 탭으로 구분되는 것을 의미한다.
quoting =3 은 쌍따옴표를 무시하도록 한다.
"""
# QUOTE_MINIMAL (0), QUOTE_ALL (1),
# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).

# 레이블인 sentiment 가 있는 학습 데이터
train = pd.read_csv('data/labeledTrainData.tsv',header=0,delimeter='\t',quoting=3)

# 레이블이 없는 테스트 데이터
test = pd.read_csv('data/testData.tsv',header=0,delimeter='\t',quoting=3)

#train 데이터의 shape출력
print(train.shape)

#train 데이터 상위 5개만 출력
print(train.head())

#train 데이터 하위 5개만 출력
print(train.tail())

#test 데이터 shape출력
print(test.shape())

#test 데이터 상위 5개만 출력
print(test.head())

#train 데이터 컬럼값 확인
print(train.columns.values)

#train 데이터 info() 확인
print(train.info())

#train 데이터 describe() 확인
print(train.describe())

#train 데이터 value)counts()
print(train['sentiment'].value_counts())

#html 태그가 섞여있기 때문에 이를 정제할 필요가 있음
print(train['review'][0][:700])


```



## 데이터 정제 Data Cleaning and Text Preprocessing

: 기계가 텍스트를 이해할 수 있도록 텍스트를 정제해준다.

신호와 소음을 구분한다. 아웃라이어 데이터로 인한 오버피팅을 방지한다.



<hr>

##### 정제 순서는 다음과 같다.

1. BeautifulSoup을 통해 html태그를 제거
2. 정규표현식으로 알파벳 이외의 문자를 공백으로 치환
3. NLTK 데이터를 사용해 불용어(Stopword)를 제거
4. 어간추출(스테밍 Stemming)과 음소표기법(Lemmatizing)의 개념을 이해하고 SnowballStemmer를 통해 어간을 추출

<hr>

## 텍스트 데이터 전처리 이해하기

https://github.com/twitter/twitter-korean-text

(출처 : 트위터 한국어 형태소 분석기)

#### 정규화 normalization(입니닼ㅋㅋ->입니다 ㅋㅋ, 샤릉해 -> 사랑해)

* 한국어를 처리하는 예시입니닼ㅋㅋㅋㅋㅋㅋㅋ -> 한국어를 처리하는 예시입니다 ㅋㅋ



#### 토큰화 tokenization

* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어Noun, 를 Josa, 처리Noun, 하는Verb, 예시Noun, 입Adjective, 니다Eomi ㅋㅋKoreanParicle



#### 어근화 stemming(입니다 -> 이다)

* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어Noun, 를Josa, 처리Noun, 하다Verb, 예시Noun, 이다Adjective, ㅋㅋKoreanParticle



#### 어구 추출 phrase extraction

* 한국어를 처리하는 예시입니다 ㅋㅋ -> 한국어, 처리, 예시, 처리하는 예시



한국어로 데이터를 텍스트 전처리 할때는 KoNLPy 를 사용하고 트위터 형태소 분석기로 만들어졌음.



## BeautifulSoup로 html 태그 제거하기

```python
from bs4 import BeautifulSoup

example1 = BeautifulSoup(train['review'][0],"html5lib")
print(train['review'][0][:700])
print(example1.get_text()[:700])

```



## 정규표현식을 사용해서 특수문자를 제거

```python
import re
#소문자와 대문자가 아닌 것은 공백으로 대체한다.
letters_only = re.sub('[^a-zA-z]',' ',example.get_text())
print(letters_only[:700])
```



## 모두 소문자로 변환

```python
lower_case = letters_only.lower()
#문자를 나눈다 . => 토큰화
words = lower_case.split()
print(len(words))
print(words[:10])

```



## 불용어 제거(Stopword Removal)

: 일반적으로 코퍼스에서 자주 나타나는 단어는 학습 모델로서 학습이나 예측 프로세스에 실제로 기여하지 않아 다른 텍스트와 구분하지 못한다. 예를 들어 조사, 접미사, i,me,my,it,this,that,is,are 등과 같은 단어는 빈번하게 등장하지만 실제 의미를 찾는데 큰 기여를 하지 않는다.

Stopwords는 "to" 또는 "the"와 같은 용어를 포함하므로 사전 처리 단계에서 제거하는 것이 좋다. 

NLTK 에는 153개의 영어 불용어가 미리 정의되어 있다. 17개의 언어에 대해 정의되어 있으며 한국어는 없다.



## NLTK data 설치

http://corazzon.github.io/nltk_data_install



## 불용어(Stopwords) 제거하기

```python
import nltk
from nltk.corpus import stopwords
print(stopwords.words('english'))
```



## Stopwords 를 제거한 토큰들

```python
tokens =[]
for w in words:
    if not w in stopwords.words('english'):
        tokens.append(w)
print(len(tokens))
print(tokens[:10])
```



## 스테밍(어간추출, 형태소 분석)

출처 : 위키백과

**어간 추출**(語幹 抽出, 영어: stemming)은 형태론 및 정보 검색 분야에서 어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 **어간**을 분리해 내는 것을 의미한다.

다음은 [영어](https://ko.wikipedia.org/wiki/%EC%98%81%EC%96%B4) 단어에 대한 스테머의 동작 예시이다. 

* 문자열 “cats”(“catlike”, “catty” 등도 마찬가지)의 어간으로는 “cat”이 추출된다. 

* “stemmer”, “stemming”, “stemmed”의 어간은 “stem”이다. 

* “fishing”, “fished”, “fisher”는 “fish”가 된다. 

* “argue”, “argued”, “arguing”, “argus”의 어간은 “argu”이다.(추출된 어간이 어근이나 단어의 원형과 일치하지 않는 경우) 

* 그러나 “argument”, “arguments”에서는 “argument”가 추출된다.



### stemming(형태소 분석) 

: 여기에서는 NLTK에서 제공하는 형태소 분석기를 사용한다. 포터 형태소 분석기는 보수적이고 랭커스터 형태소 분석기는 좀 더 적극적이다. 형태소 분석 규칙의 적극성 때문에 랭커스터 형태소 분석기는 더 많은 동음이의어 형태소를 생산한다.



### PorterStemmer

```python
#포터 스테머 사용 예
stemmer = nltk.stem.PorterStemmer()
print(stemmer.stem('maximum))
print("The stemmed form of running is : {}".format(stemmer.stem("running")))
print("The stemmed form of runs is : {}".format(stemmer.stem("runs")))
print("The stemmed form of run is : {}".format(stemmer.stem("run")))
```

```
maximum
The stemmed form of running is : run
The stemmed form of runs is : run
The stemmed form of run is : run
```

### LancasterStemmer

```python
#랭커 스테머의 사용 예
from nltk.stem.lancaster import LancasterStemmer
lancaster_stemmer = LancasterStemmer()
print(lancaster_stemmer.stem("maximum"))
print("The stemmed form of running is : {}".format(lancaster_stemmer.stem("running")))
print("The stemmed form of runs is : {}".format(lancaster_stemmer.stem("runs")))
print("The stemmed form of run is : {}".format(lancaster_stemmer.stem("run")))
```

```
maxim
The stemmed form of running is : run
The stemmed form of runs is : run
The stemmed form of run is : run
```



## 스테밍 하기 전 토큰들

```python
print(tokens[:10])
```



## Snowball 스테머로 정제하기

```python
from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer('english')
tokens = [stemmer.stem(w) for w in tokens]
print(tokens[:10])
```



## Lemmatization 음소표기법

언어학에서 음소 표기법 (또는 lemmatization)은 단어의 보조 정리 또는 사전 형식에 의해 식별 되는 단일 항목으로 분석될 수 있도록 굴절된 형태의 단어를 그룹화 하는 과정이다. 예를 들어, 동음이의어가 문맥에 따라 다른 의미를 갖는데

1. 배가 맛있다.
2. 배를 타는 것이 재미있다.
3. 평소보다 두 배로 많이 먹어서 배가 아프다.



위에 있는 3개의 문장에 있는 "배"는 모두 다른 의미를 갖는다.

레마타이제이션은 이 때 앞뒤 문맥을 보고 단어의 의미를 식별하는 것이다. 영어에서 meet는 meeting으로 쓰였을 때, 회의를 뜻하지만 meet일 때는 만나다는 뜻을 갖는데 그 단어가 명사로 쓰였는지 동사로 쓰였는지에 따라 적합한 의미를 갖도록 추출하는 것이다.

* 참고:
  * Stemming and lemmatization
  * Lemmatization - Wikipedia

```python
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

print(wordnet_lemmatizer.lemmatize('fly'))
print(wordnet_lemmatizer.lemmatize('flies'))

tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens]
tokens[:10]

```



## 문자열 처리

* 위에서 간략하게 살펴 본 내용을 바탕으로 문자열 처리해본다.

```python
def review_to_words(raw_review):
    #1. HTML 제거
    review_text = BeautifulSoup(raw_review,'html.parser').get_text()
    #2. 영문자가 아닌 문자는 공백으로 변환
    letters_only = re.sub('[^a-zA-Z]',' ',review_text)
    #3. 소문자 변환
    words = letters_only.lower().split()
    #4. 파이썬에서는 리스트보다 세트로 찾는게 훨씬 빠르다.
    # stopwords 를 세트로 변환한다.
    stops = set(stopwords.words('english'))
    # 5. Stopwords 불용어 제거
    meaningful_words = [w for w in words if not w in stops]
    # 6. 어간 추출
    stemming_words = [stemmer.stem(w) for w in meaningful_words]
    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환
    return( ' ',join(stemming_words))
```

```python
clean_review = review_to_words(train['review'][0])
```

